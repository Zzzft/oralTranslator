{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "968b3659-a0f4-4dd9-866e-936cd3b3512c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-09T05:45:23.543200Z",
     "iopub.status.busy": "2025-04-09T05:45:23.542943Z",
     "iopub.status.idle": "2025-04-09T05:45:41.114120Z",
     "shell.execute_reply": "2025-04-09T05:45:41.113630Z",
     "shell.execute_reply.started": "2025-04-09T05:45:23.543185Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668f37838ef349d98a9f299efe35cb4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 13:45:30.188769: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-09 13:45:31.118634: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-09 13:45:32.480806: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig \n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e4b2293-3762-4b5c-939b-2dab7a0955ae",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-04-09T05:45:41.122195Z",
     "iopub.status.busy": "2025-04-09T05:45:41.121987Z",
     "iopub.status.idle": "2025-04-09T05:45:41.163269Z",
     "shell.execute_reply": "2025-04-09T05:45:41.162862Z",
     "shell.execute_reply.started": "2025-04-09T05:45:41.122180Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': ['请你将以下老年人口语转换成书面语。',\n",
       "  '请你将以下老年人口语转换成书面语。',\n",
       "  '请你将以下老年人口语转换成书面语。'],\n",
       " 'input': ['给我打电话，我说我尿不湿刚给买完就在旁边椅子上搁着',\n",
       "  '有时候挣的比工资比较高一些，他也经常给老太太买点什么的，老太太爱喝点什么酒什么的，点心什么的，他也经常去买。',\n",
       "  '她也有儿有女有孙子有孙女。她就在西南楼南楼。'],\n",
       " 'output': ['他给我打电话时，我告诉他我刚买好尿不湿，就放在旁边的椅子上了。',\n",
       "  '老三收入较高，经常给母亲买些她爱喝的酒和点心。',\n",
       "  '她育有子女和孙辈，住在西南楼南楼一带。'],\n",
       " '__index_level_0__': [361, 73, 374]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 导入训练数据\n",
    "df = pd.read_json('/mnt/workspace/dataset/train.json')\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "\n",
    "train_ds[:3]\n",
    "val_ds[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56aa4253-a3b1-441d-b24f-88a78d1732d1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-09T05:45:41.163908Z",
     "iopub.status.busy": "2025-04-09T05:45:41.163720Z",
     "iopub.status.idle": "2025-04-09T05:45:41.581655Z",
     "shell.execute_reply": "2025-04-09T05:45:41.581130Z",
     "shell.execute_reply.started": "2025-04-09T05:45:41.163894Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2Tokenizer(name_or_path='/mnt/workspace/tmp/Qwen/Qwen2___5-7B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/mnt/workspace/tmp/Qwen/Qwen2___5-7B-Instruct', use_fast=False, trust_remote_code=True)\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12df8a44-3ab8-4a19-8b39-967cb76d0709",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-09T05:45:41.584860Z",
     "iopub.status.busy": "2025-04-09T05:45:41.584534Z",
     "iopub.status.idle": "2025-04-09T05:45:41.589021Z",
     "shell.execute_reply": "2025-04-09T05:45:41.588613Z",
     "shell.execute_reply.started": "2025-04-09T05:45:41.584845Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 384    # Llama分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(f\"<|im_start|>system\\n你是一位老年服务机构的文书编辑，擅长将老人的口头叙述准确、清晰地转化为正式文档。<|im_end|>\\n<|im_start|>user\\n{example['instruction'] + example['input']}<|im_end|>\\n<|im_start|>assistant\\n\", add_special_tokens=False)  # add_special_tokens 不在开头加 special_tokens\n",
    "    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # 因为eos token咱们也是要关注的所以 补充为1\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]  \n",
    "    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7319b6c4-e5d0-47da-9dea-4f28f82e01dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T05:45:41.589726Z",
     "iopub.status.busy": "2025-04-09T05:45:41.589492Z",
     "iopub.status.idle": "2025-04-09T05:45:47.132181Z",
     "shell.execute_reply": "2025-04-09T05:45:47.131761Z",
     "shell.execute_reply.started": "2025-04-09T05:45:41.589706Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26cbda2b24704555a5b3be602be20952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/450 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5cdeea6aee244539ca7adf3afe4657a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = train_ds.map(process_func, remove_columns=train_ds.column_names)\n",
    "tokenized_val = val_ds.map(process_func, remove_columns=val_ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c2071a6-b31b-4a5e-9b21-bdd6d9ae41c9",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-09T05:45:47.132910Z",
     "iopub.status.busy": "2025-04-09T05:45:47.132690Z",
     "iopub.status.idle": "2025-04-09T05:45:47.136847Z",
     "shell.execute_reply": "2025-04-09T05:45:47.136455Z",
     "shell.execute_reply.started": "2025-04-09T05:45:47.132895Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n你是一位老年服务机构的文书编辑，擅长将老人的口头叙述准确、清晰地转化为正式文档。<|im_end|>\\n<|im_start|>user\\n请你将以下老年人口语转换成书面语。当然老三也不错，老三他在公安系统他有这个条件。<|im_end|>\\n<|im_start|>assistant\\n当然老三也不错，他在公安系统工作，具备这样的条件。<|endoftext|>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_train[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bc95adb-6b98-4378-bf90-863c5e367491",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-09T05:45:47.137532Z",
     "iopub.status.busy": "2025-04-09T05:45:47.137349Z",
     "iopub.status.idle": "2025-04-09T05:45:47.141022Z",
     "shell.execute_reply": "2025-04-09T05:45:47.140563Z",
     "shell.execute_reply.started": "2025-04-09T05:45:47.137517Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'当然老三也不错，他在公安系统工作，具备这样的条件。<|endoftext|>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(list(filter(lambda x: x != -100, tokenized_train[0][\"labels\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daac681e-015b-4412-8c9f-9da8361d28de",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-09T05:45:47.142210Z",
     "iopub.status.busy": "2025-04-09T05:45:47.141910Z",
     "iopub.status.idle": "2025-04-09T05:46:33.003801Z",
     "shell.execute_reply": "2025-04-09T05:46:33.003064Z",
     "shell.execute_reply.started": "2025-04-09T05:45:47.142197Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e951a298a5e64a4d9f3c2f907411b500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    '/mnt/workspace/tmp/Qwen/Qwen2___5-7B-Instruct/', \n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_cache=False\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63124620-6f4f-4020-bfee-7be44aedc805",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T05:46:33.004829Z",
     "iopub.status.busy": "2025-04-09T05:46:33.004520Z",
     "iopub.status.idle": "2025-04-09T05:46:33.007738Z",
     "shell.execute_reply": "2025-04-09T05:46:33.007296Z",
     "shell.execute_reply.started": "2025-04-09T05:46:33.004806Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.enable_input_require_grads() # 开启梯度检查点时，要执行该方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4903968-c561-40d8-b450-a5a7ec231037",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T05:46:33.008554Z",
     "iopub.status.busy": "2025-04-09T05:46:33.008358Z",
     "iopub.status.idle": "2025-04-09T05:46:33.011342Z",
     "shell.execute_reply": "2025-04-09T05:46:33.010937Z",
     "shell.execute_reply.started": "2025-04-09T05:46:33.008539Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9fbb37-ccff-42ef-aa60-6e01c10084ae",
   "metadata": {},
   "source": [
    "# lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45ab5181-b560-42f5-b485-991feacf8779",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-04-09T05:46:49.840837Z",
     "iopub.status.busy": "2025-04-09T05:46:49.840533Z",
     "iopub.status.idle": "2025-04-09T05:46:49.844844Z",
     "shell.execute_reply": "2025-04-09T05:46:49.844406Z",
     "shell.execute_reply.started": "2025-04-09T05:46:49.840822Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=32, target_modules={'v_proj', 'q_proj', 'up_proj', 'k_proj', 'o_proj', 'down_proj', 'gate_proj'}, exclude_modules=None, lora_alpha=64, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False, # 训练模式\n",
    "    r=32, # Lora 秩\n",
    "    lora_alpha=64, # Lora alaph\n",
    "    lora_dropout=0.1 # Dropout 比例\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73403447-e6f8-436d-a6ae-9a3e62d697dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T05:46:51.794726Z",
     "iopub.status.busy": "2025-04-09T05:46:51.794422Z",
     "iopub.status.idle": "2025-04-09T05:46:53.884758Z",
     "shell.execute_reply": "2025-04-09T05:46:53.883966Z",
     "shell.execute_reply.started": "2025-04-09T05:46:51.794708Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/mnt/workspace/tmp/Qwen/Qwen2___5-7B-Instruct/', revision=None, inference_mode=False, r=32, target_modules={'v_proj', 'q_proj', 'up_proj', 'k_proj', 'o_proj', 'down_proj', 'gate_proj'}, exclude_modules=None, lora_alpha=64, lora_dropout=0.1, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_peft_model(model, config)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdadeda7-40c7-4d13-bb10-2342f2589b59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T05:46:53.886083Z",
     "iopub.status.busy": "2025-04-09T05:46:53.885791Z",
     "iopub.status.idle": "2025-04-09T05:46:53.894585Z",
     "shell.execute_reply": "2025-04-09T05:46:53.893972Z",
     "shell.execute_reply.started": "2025-04-09T05:46:53.886062Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 80,740,352 || all params: 7,696,356,864 || trainable%: 1.0491\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bcf2d8-27ba-46e7-afc3-0af4564f0bcd",
   "metadata": {},
   "source": [
    "# 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2ebb791-793f-43c4-95a1-25b5d5e0b070",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-04-09T05:46:56.433162Z",
     "iopub.status.busy": "2025-04-09T05:46:56.432827Z",
     "iopub.status.idle": "2025-04-09T05:46:56.446520Z",
     "shell.execute_reply": "2025-04-09T05:46:56.446090Z",
     "shell.execute_reply.started": "2025-04-09T05:46:56.433146Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"/mnt/workspace/output/Qwen2.5-7B-Instruct\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=100, # 为了快速演示，这里设置10，建议你设置成100\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_on_each_node=True,\n",
    "    eval_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9daa30c2-752c-49db-888a-af48baaa434e",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-09T05:47:00.363327Z",
     "iopub.status.busy": "2025-04-09T05:47:00.363046Z",
     "iopub.status.idle": "2025-04-09T05:47:02.320657Z",
     "shell.execute_reply": "2025-04-09T05:47:02.320178Z",
     "shell.execute_reply.started": "2025-04-09T05:47:00.363312Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-09 13:47:00,454] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "df: /root/.triton/autotune: 没有那个文件或目录\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d051dc78-00a8-4d89-aa36-7ed70b5bf71b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T05:47:04.352118Z",
     "iopub.status.busy": "2025-04-09T05:47:04.351801Z",
     "iopub.status.idle": "2025-04-09T05:51:06.634065Z",
     "shell.execute_reply": "2025-04-09T05:51:06.633607Z",
     "shell.execute_reply.started": "2025-04-09T05:47:04.352102Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [84/84 03:56, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.545100</td>\n",
       "      <td>1.344462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.111700</td>\n",
       "      <td>1.309181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=84, training_loss=1.3798025023369562, metrics={'train_runtime': 241.9038, 'train_samples_per_second': 5.581, 'train_steps_per_second': 0.347, 'total_flos': 6460681188648960.0, 'train_loss': 1.3798025023369562, 'epoch': 2.920353982300885})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5574473-4b7b-402b-b396-e32dd72fc6f8",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-04-09T05:51:06.635046Z",
     "iopub.status.busy": "2025-04-09T05:51:06.634829Z",
     "iopub.status.idle": "2025-04-09T05:51:06.651303Z",
     "shell.execute_reply": "2025-04-09T05:51:06.650880Z",
     "shell.execute_reply.started": "2025-04-09T05:51:06.635029Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 导入测试数据\n",
    "df = pd.read_json('/mnt/workspace/dataset/test.json')\n",
    "\n",
    "test_ds = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2406a20b-83f8-49ba-a8d2-41f6cddd7a16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T05:51:06.651984Z",
     "iopub.status.busy": "2025-04-09T05:51:06.651848Z",
     "iopub.status.idle": "2025-04-09T05:51:09.176319Z",
     "shell.execute_reply": "2025-04-09T05:51:09.175939Z",
     "shell.execute_reply.started": "2025-04-09T05:51:06.651971Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d01a672a4974c3e94462db9019b0d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_test = test_ds.map(process_func, remove_columns=test_ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3ae3fbe8-de87-41b6-a1ee-ed10e93ce7fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T05:51:09.177272Z",
     "iopub.status.busy": "2025-04-09T05:51:09.177068Z",
     "iopub.status.idle": "2025-04-09T05:51:13.788947Z",
     "shell.execute_reply": "2025-04-09T05:51:13.788473Z",
     "shell.execute_reply.started": "2025-04-09T05:51:09.177260Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.3284080028533936,\n",
       " 'eval_runtime': 4.595,\n",
       " 'eval_samples_per_second': 21.545,\n",
       " 'eval_steps_per_second': 2.829,\n",
       " 'epoch': 2.920353982300885}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(tokenized_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0814a560-1e42-4ced-99e6-de9648f8bfd8",
   "metadata": {},
   "source": [
    "# 合并加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7219bb51-66fa-4dae-bf5a-997382c220df",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-04-09T05:52:34.493536Z",
     "iopub.status.busy": "2025-04-09T05:52:34.493233Z",
     "iopub.status.idle": "2025-04-09T05:52:42.683840Z",
     "shell.execute_reply": "2025-04-09T05:52:42.683189Z",
     "shell.execute_reply.started": "2025-04-09T05:52:34.493520Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 128 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141e806c498841538034f7a42c09fb7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/accelerate/utils/modeling.py:1569: UserWarning: Current model requires 256 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "mode_path = '/mnt/workspace/tmp/Qwen/Qwen2___5-7B-Instruct/'\n",
    "lora_path = '/mnt/workspace/output/Qwen2.5-7B-Instruct/checkpoint-84/' # 这里改称 lora 输出对应 checkpoint 地址\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_path, trust_remote_code=True)\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(mode_path, device_map=\"auto\",torch_dtype=torch.bfloat16, trust_remote_code=True).eval()\n",
    "\n",
    "# 加载lora权重\n",
    "model = PeftModel.from_pretrained(model, model_id=lora_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1224c14-304b-4240-96f3-8a2a9609d0ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T05:52:42.685077Z",
     "iopub.status.busy": "2025-04-09T05:52:42.684871Z",
     "iopub.status.idle": "2025-04-09T05:52:42.688091Z",
     "shell.execute_reply": "2025-04-09T05:52:42.687616Z",
     "shell.execute_reply.started": "2025-04-09T05:52:42.685063Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "{'instruction': '请你将以下老年人口语转换成书面语。', 'input': '过去都兴外协件给加工拿来活，一般人都不接不干，干不了。', 'output': '过去工厂常将外协加工任务外包，但一般人都不敢接，因为难以胜任。'}\n"
     ]
    }
   ],
   "source": [
    "print(len(test_ds))  # 检查测试集样本数量\n",
    "print(test_ds[0])   # 查看第一条数据内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e3d99a0a-50c7-4daf-9c68-c0e35a99ae73",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-04-09T05:52:42.688776Z",
     "iopub.status.busy": "2025-04-09T05:52:42.688585Z",
     "iopub.status.idle": "2025-04-09T06:10:26.842678Z",
     "shell.execute_reply": "2025-04-09T06:10:26.842160Z",
     "shell.execute_reply.started": "2025-04-09T05:52:42.688764Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sample 1/50\n",
      "Processing sample 2/50\n",
      "Processing sample 3/50\n",
      "Processing sample 4/50\n",
      "Processing sample 5/50\n",
      "Processing sample 6/50\n",
      "Processing sample 7/50\n",
      "Processing sample 8/50\n",
      "Processing sample 9/50\n",
      "Processing sample 10/50\n",
      "Processing sample 11/50\n",
      "Processing sample 12/50\n",
      "Processing sample 13/50\n",
      "Processing sample 14/50\n",
      "Processing sample 15/50\n",
      "Processing sample 16/50\n",
      "Processing sample 17/50\n",
      "Processing sample 18/50\n",
      "Processing sample 19/50\n",
      "Processing sample 20/50\n",
      "Processing sample 21/50\n",
      "Processing sample 22/50\n",
      "Processing sample 23/50\n",
      "Processing sample 24/50\n",
      "Processing sample 25/50\n",
      "Processing sample 26/50\n",
      "Processing sample 27/50\n",
      "Processing sample 28/50\n",
      "Processing sample 29/50\n",
      "Processing sample 30/50\n",
      "Processing sample 31/50\n",
      "Processing sample 32/50\n",
      "Processing sample 33/50\n",
      "Processing sample 34/50\n",
      "Processing sample 35/50\n",
      "Processing sample 36/50\n",
      "Processing sample 37/50\n",
      "Processing sample 38/50\n",
      "Processing sample 39/50\n",
      "Processing sample 40/50\n",
      "Processing sample 41/50\n",
      "Processing sample 42/50\n",
      "Processing sample 43/50\n",
      "Processing sample 44/50\n",
      "Processing sample 45/50\n",
      "Processing sample 46/50\n",
      "Processing sample 47/50\n",
      "Processing sample 48/50\n",
      "Processing sample 49/50\n",
      "Processing sample 50/50\n"
     ]
    }
   ],
   "source": [
    "### 生成测试结果\n",
    "\n",
    "from eval import evaluate_generation\n",
    "import random\n",
    "# 随机选择50条测试数据（不重复采样）\n",
    "random.seed(42)  # 固定随机种子保证可复现 42\n",
    "test_samples = random.sample(list(test_ds), min(50, len(test_ds)))\n",
    "\n",
    "# 生成预测结果\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "for i, example in enumerate(test_samples):\n",
    "    print(f\"Processing sample {i+1}/{len(test_samples)}\")  # 打印进度\n",
    "    # 生成回答\n",
    "    inputs = tokenizer(\n",
    "        f\"<|im_start|>system\\n你是一位老年服务机构的文书编辑，擅长将老人的口头叙述准确、清晰地转化为正式文档。<|im_end|>\\n\"\n",
    "        f\"<|im_start|>user\\n{example['instruction']}\\n{example['input']}<|im_end|>\\n\"\n",
    "        f\"<|im_start|>assistant\\n\",\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "    outputs = model.generate(**inputs, \n",
    "                             max_new_tokens=200, \n",
    "                             num_beams=1, # 禁用束搜索（加速）\n",
    "                             do_sample=True, # 采样\n",
    "                             temperature=0.3,      # 温度 0.1~1.5\n",
    "                             top_p=0.95,            # top-p 0.7~0.95\n",
    "                            )\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    predictions.append(pred)\n",
    "    references.append([example['output']])  # 注意references需要是列表的列表\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f958715-7eb4-4d03-85cb-186656b3457b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T06:10:26.844601Z",
     "iopub.status.busy": "2025-04-09T06:10:26.843676Z",
     "iopub.status.idle": "2025-04-09T06:10:26.848329Z",
     "shell.execute_reply": "2025-04-09T06:10:26.847675Z",
     "shell.execute_reply.started": "2025-04-09T06:10:26.844569Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我挑选了一款很小号且加长的护垫，并拍摄了照片提交给他。'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a716568-62f1-405c-95fc-54fcb42b1802",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T06:10:26.849030Z",
     "iopub.status.busy": "2025-04-09T06:10:26.848861Z",
     "iopub.status.idle": "2025-04-09T06:10:26.852236Z",
     "shell.execute_reply": "2025-04-09T06:10:26.851769Z",
     "shell.execute_reply.started": "2025-04-09T06:10:26.849018Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我告诉他这是很小的护垫，但算是加长款，并拍照发给了他。'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e2fd0707-661d-45d4-ae65-ee20a7e2540b",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-04-09T06:10:34.167919Z",
     "iopub.status.busy": "2025-04-09T06:10:34.167505Z",
     "iopub.status.idle": "2025-04-09T06:10:34.182252Z",
     "shell.execute_reply": "2025-04-09T06:10:34.181821Z",
     "shell.execute_reply.started": "2025-04-09T06:10:34.167892Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import eval\n",
    "importlib.reload(eval)  # 强制重新加载\n",
    "from eval import *  # 再尝试导入eval函数\n",
    "\n",
    "import nltk\n",
    "# 设置 NLTK 数据存储路径（可以是任意目录）\n",
    "nltk_data_dir = \"/mnt/workspace/nltk_data\"\n",
    "\n",
    "# 确保目录存在\n",
    "import os\n",
    "os.makedirs(nltk_data_dir, exist_ok=True)\n",
    "\n",
    "# 将该路径添加到 NLTK 的数据搜索路径\n",
    "nltk.data.path.append(nltk_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db26e93a-8476-40f7-8115-eb898cdf34b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T06:10:36.196668Z",
     "iopub.status.busy": "2025-04-09T06:10:36.196355Z",
     "iopub.status.idle": "2025-04-09T06:10:39.300671Z",
     "shell.execute_reply": "2025-04-09T06:10:39.300071Z",
     "shell.execute_reply.started": "2025-04-09T06:10:36.196652Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.628 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我 挑选 了 一款 很小 号且 加长 的 护垫 ， 并 拍摄 了 照片 提交 给 他 。\n",
      "[['我 告诉 他 这 是 很小 的 护垫 ， 但 算是 加长 款 ， 并 拍照 发给 了 他 。']]\n",
      "------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 16.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每次 见到 那位 律师 时 ， 我 总 忍不住 向 她 倾诉 心中 的 苦楚 ， 因为 实在 无法 向 其他人 诉说 。\n",
      "[['现在 每次 见到 律师 ， 我 都 想 倾诉 苦衷 ， 但 这些 话 实在 无法 向 他人 诉说 。']]\n",
      "------------------\n",
      "他 平时 喜欢 做些 需要 动手 动脑 的 事情 ， 但 性格 上 却 不 太 让 人 省心 。\n",
      "[['其他 事情 他 都 喜欢 亲力亲为 追求 细致 ， 但 做事 不够 稳妥 。']]\n",
      "------------------\n",
      "起初 我 没有 注意 到 ， 打开 后 就 随手 把 纸盒 扔进 了 小区 的 垃圾桶 。\n",
      "[['我 解释 说 一 开始 没 留意 ， 拆开 后 就 把 纸盒 扔进 垃圾桶 了 ， 后来 丢到 了 小区 的 公共 垃圾桶 里 。']]\n",
      "------------------\n",
      "2008 年 后 ， 他 的 行动 变得 非常 不便 ， 我们 一直 在 家里 照顾 他 。\n",
      "[['2008 年 后 ， 母亲 行动 越来越 不便 ， 一直 由 我们 在家 照料 。']]\n",
      "------------------\n",
      "当时 确实 耽误 了 ， 每次 看到 她 痛苦 的 样子 ， 我 都 实在 不忍心 再 让 她 受折磨 。\n",
      "[['在 那 期间 ， 治疗 被 耽误 了 。 看到 他 痛苦不堪 的 样子 ， 我 也 不忍心 再 继续 折腾 他 了 。']]\n",
      "------------------\n",
      "说 实在话 ， 他常去 有名 的 老字号 餐馆 吃饭 ， 还 和 两位 老朋友 一起 经常 去 北京 ， 品尝 正宗 的 北京烤鸭 。\n",
      "[['说实话 ， 他 经常 去 有名 的 餐馆 吃饭 。 那 时候 还有 两位 老人 和 他 一起 ， 经常 专程去 北京 吃 烤鸭 。']]\n",
      "------------------\n",
      "爷爷 摔伤 后 ， 2009 年 如果 我 去 外地 打工 ， 还 能 挣 不少 钱 。 当时 单位 有 岗位 空缺 ， 正 需要 我 的 专业技能 ， 但 因为 爷爷 摔断了 股骨头 ， 我 留在 家里 照顾 他 ， 所以 没有 外出 工作 。\n",
      "[['2009 年 爷爷 股骨 骨折 后 ， 原本 有 同事 邀请 我 外出 工作 ， 我 的 专业技能 正是 他们 需要 的 岗位 ， 收入 也 很 可观 。 但 为了 照顾 骨折 卧床 的 爷爷 ， 我 最终 放弃 了 这次 工作 机会 。']]\n",
      "------------------\n",
      "嫁给 他 后 ， 我 至今 一直 过得 不如意 。\n",
      "[['自从 嫁给 他 ， 我 便 一直 不幸 至今 。']]\n",
      "------------------\n",
      "我 明确 表示 自己 订购 的 是 卫生巾 ， 而 不是 那 款 尺寸 过 小 的 护垫 。\n",
      "[['我 订购 的 是 卫生巾 ， 不是 护垫 ， 这个 护垫 太小 了 。']]\n",
      "------------------\n",
      "哎呀 ， 这 完全 不 对 ， 商品 与 描述 不符 ， 但 自己 却 不知 如何 维权 。\n",
      "[['这 完全 不 对 ， 货不对 板 ， 但 自己 却 不 知道 主动 维护 自己 的 权益 。']]\n",
      "------------------\n",
      "我们 俩 在 同一个 车间 工作 ， 是 通过 他 技校 的 同学 介绍 认识 的 。\n",
      "[['我们 算不上 自由恋爱 ， 在 同一个 车间 工作 ， 由 他 技校 时期 的 同学 介绍 相识 。']]\n",
      "------------------\n",
      "接收 到 遗体 后 ， 我 当时 确实 没有 太 在意 。\n",
      "[['东西 送到 后 ， 我 当时 确实 没太 在意 。']]\n",
      "------------------\n",
      "我 只 说 了 她 是 个 寡妇 ， 我 不会 责怪 她 。\n",
      "[['我 并 没有 多 说 什么 ， 只 说 了 一句 她 是 寡妇 ， 我 并 没有 辱骂 她 。']]\n",
      "------------------\n",
      "从那时起 至今 ， 尤其 是 照顾 完 爷爷 后 ， 我 整夜 无法 入睡 。\n",
      "[['直到现在 ， 尤其 是 自从 照顾 爷爷 以来 ， 昼夜 颠倒 无法 正常 休息 。']]\n",
      "------------------\n",
      "关于 护垫 的 问题 ， 如果 不 需要的话 ， 您 随意 处理 即可 。\n",
      "[['我 询问 护垫 该 如何 处理 ， 对方 表示 如果 不 需要 护垫 ， 可以 自行 丢弃 或 随意 处置 。']]\n",
      "------------------\n",
      "后来 他 不管不顾 ， 一直 纠缠 我 ， 反复 询问 具体 尺寸 和 细节 。\n",
      "[['之后 他 不再 理会 其他人 ， 转而 一直 纠缠 我 ， 反复 询问 长度 等 细节 ， 不断 纠缠 这个 问题 。']]\n",
      "------------------\n",
      "以前 我们 经常 和 楼下 的 老 邻居们 一起 去 饭店 聚会 ， 但 这样 存 不了 多少 钱 。\n",
      "[['那 时候 附近 的 老年人 经常 下馆子 、 出去玩 ， 所以 存 不下 什么 钱 。']]\n",
      "------------------\n",
      "您 曾多次 前往 蓟县 旅游 ， 总共 去 了 七次 ， 每次 都 玩 得 很 尽兴 。\n",
      "[['他 只要 想 去 蓟县 旅游 ， 就 会 去 玩 一趟 ， 前前后后 总共 去 了 七次 。']]\n",
      "------------------\n",
      "有时 会 遇到 送货 错误 或 货物 损坏 的 情况 。\n",
      "[['偶尔 会 有 送错 货 的 情况 ， 或者 发错 商品 ， 甚至 收到 损坏 的 商品 。']]\n",
      "------------------\n",
      "收到 货物 后 ， 我 随即 拆开 包装盒 ， 并 将 其 丢 进 了 垃圾桶 。\n",
      "[['收到 货物 后 ， 我 拆开 包装盒 便 顺手 扔进 了 垃圾桶 。']]\n",
      "------------------\n",
      "他 询问 我 那个 装货 的 盒子 尺寸 是 多少 ， 具体来说 就是 长 多少 、 宽 多少 、 高 多少 。\n",
      "[['对方 询问 发货 用 的 盒子 尺寸 ， 包括 长 、 宽 、 高 各 是 多少 ， 并 问 我 装货 的 盒子 尺寸 。']]\n",
      "------------------\n",
      "有 一次 我 在 淘宝 上 购买 卫生巾 ， 虽然 我 自己 已经 不 需要 了 ， 但 打算 给 晚辈 使用 。\n",
      "[['有 一次 在 淘宝 上 买 了 卫生巾 ， 当然 我 自己 不用 ， 是 替 小辈 买 的 。']]\n",
      "------------------\n",
      "因此 ， 我 父亲 非常 孝顺 ， 每天 早上 都 主动 起床 为 母亲 准备 早餐 。\n",
      "[['因此 ， 我 父亲 非常 顺从 ， 从那以后 每天 早晨 一 起床 ， 他 就 给 我 买 吃 的 。']]\n",
      "------------------\n",
      "我 解释 说 ， 只有 盒子 无法 拍摄 照片 ， 其他 物品 都 可以 拍摄 。\n",
      "[['这个 盒子 我 无法 拍摄 ， 其他 物品 都 可以 提供 照片 。']]\n",
      "------------------\n",
      "我 平时 说话 从不 骂人 。\n",
      "[['我 实在 不会 开口 骂人 。']]\n",
      "------------------\n",
      "他 检查 后 发现 是 卫生巾 ， 我 解释 说 送 的 是 护垫 ， 并且 已经 告知 过 他 。\n",
      "[['客服 查看 订单 后 发现 是 卫生巾 ， 我 解释 说 实际 需要 的 是 护垫 ， 并 再次 向 他 说明 了 情况 。']]\n",
      "------------------\n",
      "他 的 儿子 没有 工作 ， 自己 也 无业 ， 一直 和 我们 争 这笔 钱 该涨 多少 。\n",
      "[['他 的 儿子 没有 工作 ， 他 自己 也 失业 了 ， 长期 纠缠 我们 争夺 这笔 补偿金 的 涨幅 。']]\n",
      "------------------\n",
      "因此 ， 他们 处理 得 非常 迅速 ， 表示 如果 商家 不 处理 ， 他们 就 会 接手 处理 ， 并且 很快 完成 了 这件 事 。\n",
      "[['后来 平台 处理 得 非常 迅速 ， 负责人 表示 如果 商家 不 处理 ， 我们 会 负责 解决 ， 并 迅速 处理完毕 。']]\n",
      "------------------\n",
      "过去 大家 都 习惯 将 外协 件 交给 工人 加工 ， 但 一般 人 都 不愿 接手 ， 因为 实在 难以完成 。\n",
      "[['过去 工厂 常将 外协 加工 任务 外 包 ， 但 一般 人 都 不敢 接 ， 因为 难以 胜任 。']]\n",
      "------------------\n",
      "为什么 呢 ？ 之前 曾 有人 两次 向 我们 提起 此事 ， 他们 怂恿 父亲 为了 房产 分配 和 补偿款 而 闹事 ， 最终 还是 你们 兄弟俩 人 尽心尽力 地 照顾 他 直至 终老 ， 父亲 说 他 最 对不起 的 就是 你们 兄弟俩 。\n",
      "[['这是 为什么 呢 ？ 之前 其他人 曾 两次 怂恿 爷爷 争夺 房产 和 钱财 ， 但 最终 是 你们 两 人为 我 养老送终 并 照顾 我 直到 离世 ， 爷爷 说 他 最 对不住 的 就是 你们 两人 。']]\n",
      "------------------\n",
      "您 说 无法 退货 是因为 商品 有误 ， 我 不 需要 且 无法 使用 。\n",
      "[['我 向 客服 说明 无法 退货 ， 因为 您 发错 了 商品 ， 这个 商品 我 不 需要 ， 也 无法 使用 。']]\n",
      "------------------\n",
      "那时 我心 凉如水 ， 对 您 的 感情 就 像 对待 亲姐妹 一样 。\n",
      "[['你 能 体会 我们 当时 的 感受 吗 ？ 我 的 心 都 凉 透 了 。 那时 我 是 如何 对待 各位 姐姐 的 ？']]\n",
      "------------------\n",
      "这 三件 事 就 发生 过 ， 我 简单 给 大家 介绍 一下 。\n",
      "[['当时 发生 了 三件 事 ， 我 简单 介绍 一下 。']]\n",
      "------------------\n",
      "爷爷 临终前 的 最后 一句 话 是 ： “ 那时 还 能 说话 时 ， 我 最 对不起 的 就是 你们 兄弟俩 。 ”\n",
      "[['爷爷 临终前 还 能 说话 时 ， 最后 对 我们 两人 说 ： \" 我 最 感到 愧疚 的 就是 你们 两人 。 \"']]\n",
      "------------------\n",
      "有 一次 我 特别 喜欢 多肉 植物 ， 就 买 了 不少 。\n",
      "[['还有 一次 是因为 我 喜欢 多肉 植物 ， 买 了 一些 回来 。']]\n",
      "------------------\n",
      "他 当时 还 说 ： \" 即使 我 打 你 ， 也 还有 力气 剩下 。 \"\n",
      "[['他 还 说 ， 对付 你 我 还是 完全 有 能力 的 。']]\n",
      "------------------\n",
      "当年 我们 结婚 时 ， 因为 没有 房子 ， 根本无法 成家 。 那时 我 还 没有 结婚 对象 ， 没有 房子 也 找 不到 媳妇 。 母亲 让 我 结婚 ， 但 那时 我 已近 三十岁 ， 家庭 条件 也 不好 。\n",
      "[['如果 你们 不 给 我 房子 ， 我 拿 什么 买房 娶妻 ？ 那 时候 我 既 没 房子 也 没 媳妇 ， 没有 房子 根本 娶 不到 妻子 。 这 是 你 母亲 让 我 住 进去 的 ， 我 那时 都 快 30 了 还 没 成家 ， 家庭 条件 也 不好 。']]\n",
      "------------------\n",
      "我们 每天 为 他 做饭 ， 就 像 伺候 皇太后 一样 ， 无论 他 对 我 如何 不好 ， 我 每天 还是 盛 一碗 饭 给 他 吃 。\n",
      "[['我 每天 为 他 做饭 ， 依然 像 伺候 老佛爷 般 精心照料 。 无论 他 如何 待 我 不好 ， 我 仍 顿顿 做好 饭端 到 他 面前 。']]\n",
      "------------------\n",
      "爷爷 去世 后 一年 半 ， 父亲 因 高血压 引发 心率 加快 ， 心跳 达到 120 次 / 分钟 。\n",
      "[['爷爷 去世 一年 半后 ， 我 患上 了 高血压 ， 心率 超过 120 次 。']]\n",
      "------------------\n",
      "老人 有时 白天 会 打盹 ， 我们 白天 需要 工作 和 处理事务 ， 因此 他 晚上 无法 入睡 ， 作息时间 变得 混乱 。\n",
      "[['老人 白天 有时 会 睡觉 ， 而 我们 白天 需要 操持 各种 家务 ， 结果 他 夜里 又 不 睡 ， 导致 昼夜 作息 紊乱 了 。']]\n",
      "------------------\n",
      "母亲 去世 后 ， 父亲 已年 届 九旬 ， 尽量 不要 下楼 ， 如果 实在 需要 下楼 ， 我会 陪同 他 。 他 的 股骨头 曾经 摔过 ， 如果 再次 摔伤 ， 我们 六楼 的 居住 条件 无法 提供 必要 的 护理 和 治疗 。\n",
      "[['父亲 年 过 九旬 后 ， 我们 尽量 不让 他 独自 下楼 ， 必须 下楼 时 我会 陪同 。 他 股骨头 已经 摔伤 过 ， 再 摔 一次 情况 就 危险 了 ， 毕竟 我们 住 在 六楼 。']]\n",
      "------------------\n",
      "当时 测量 到 的 血压 是 120 / 160 毫米汞柱 ， 心率 是 125 次 / 分钟 。 医护人员 到场 后 立即 用 担架 将 他 送往 物业公司 ， 并 迅速 送往 医院 。\n",
      "[['当时 血压高 达 160 / 120 毫米汞柱 ， 心率 达到 每分钟 125 次 ， 急救 人员 赶到 后 立即 将 他 送往 物业 处 ， 并用 担架 抬 下楼 紧急 送医 。']]\n",
      "------------------\n",
      "每天 我 都 是 吃 一碗 饭 ， 由人 喂 水 、 喂饭 和 喂 水果 。 吃饭 时 我们 兄弟姐妹 都 围坐 一起 共享 。\n",
      "[['我 每天 自己 吃 一碗 饭 ， 再盛 一碗 给 他 。 每天 按时 给 他 喂 水 、 喂饭 、 喂 新鲜 水果 。 我 吃饭 时 总是 和 家人 一起 吃 。']]\n",
      "------------------\n",
      "以前 我 和 老伴 共同 生活 时 也 曾 遇到 过 类似 的 情况 。\n",
      "[['过去 我 和 老伴 一起 生活 时 ， 也 遇到 过 类似 情况 。']]\n",
      "------------------\n",
      "哥哥 当时 一把 掐住 他 的 脖子 ， 把 他 带到 了 我们 这里 ， 手机 里 有 当时 的 录像 为证 。\n",
      "[['因为 哥哥 一把 掐住 他 的 脖子 ， 将 他 带到 我们 那里 ， 手机 里 都 有 录像 记录 。']]\n",
      "------------------\n",
      "因此 ， 我 总共 花费 了 77 万元 ， 实际 支出 约 为 87 万元 。\n",
      "[['因此 ， 我 一共 花费 了 7 万 ， 将近 8 万元 。']]\n",
      "------------------\n",
      "现在 我 非常 后悔 。 我 下岗 已有 十年 之 久 ， 2009 年 爷爷 骑 自行车 去 洗澡时 不幸 被车撞 倒 ， 导致 股骨头 受伤 。\n",
      "[['我 如今 深感 懊悔 。 下岗 已有 十年 ， 2009 年 爷爷 骑 自行车 去 澡堂 途中 遭遇 车祸 ， 导致 股骨头 骨折 。']]\n",
      "------------------\n",
      "为了 避免 这种 情况 发生 ， 我们 不至于 现在 如此 被动 ， 我 实在 感到 委屈 ， 但 最终 还是 坚持 到 了 今天 。\n",
      "[['如果 当时 及时处理 了 这方面 的 问题 ， 我们 也 不至于 落到 现在 的 境地 。 我 感到 非常 委屈 ， 本 不该 让 我 陷入 这种 困境 。']]\n",
      "------------------\n",
      "您 说 我 辜负 了 父亲 的 期望 ， 是 吗 ？ 我 无法 尽孝 可以 理解 ， 但 虐待 他 也 是 可以 接受 的 吗 ？\n",
      "[['你 指责 我 亏待 父亲 是 吗 ？ 难道 我 不 照顾 他 可以 ， 甚至 虐待 他 也 可以 吗 ？']]\n",
      "------------------\n",
      "+--------+--------+--------+--------+--------+-----------+---------+---------+---------+--------+\n",
      "| Metric | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | sacreBLEU | ROUGE-1 | ROUGE-2 | ROUGE-L | METEOR |\n",
      "+--------+--------+--------+--------+--------+-----------+---------+---------+---------+--------+\n",
      "| Scores | 0.6602 | 0.5835 | 0.5105 | 0.4393 |   0.2562  |  0.5586 |  0.2548 |  0.5072 | 0.5642 |\n",
      "+--------+--------+--------+--------+--------+-----------+---------+---------+---------+--------+\n",
      "{'BLEU-1': 0.6602496570319868, 'BLEU-2': 0.5835252423868496, 'BLEU-3': 0.5104587007029111, 'BLEU-4': 0.43930727418611404, 'sacreBLEU': 0.25621456560305267, 'ROUGE-1': 0.5585813656657472, 'ROUGE-2': 0.25476902069516794, 'ROUGE-L': 0.5072145408096607, 'METEOR': 0.5642052554451901}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 评估\n",
    "results = evaluate_generation(\n",
    "    tokenizer=tokenizer,\n",
    "    predictions=predictions,\n",
    "    references=references,\n",
    "    intensive=True,  # 中文需要设为True\n",
    "    print_table=True\n",
    ")\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5799d4-c20f-4d2d-8569-80b79aef070c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.status.busy": "2025-04-08T05:53:32.038742Z",
     "iopub.status.idle": "2025-04-08T05:53:32.038890Z",
     "shell.execute_reply": "2025-04-08T05:53:32.038819Z",
     "shell.execute_reply.started": "2025-04-08T05:53:32.038812Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "###验证模型生成结果\n",
    "prompt = \"....\"\n",
    "inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": \"你是一位老年服务机构的文书编辑，负责将老人的口头叙述转化为正式文档。\"},{\"role\": \"user\", \"content\": prompt}],\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       return_dict=True\n",
    "                                       ).to('cuda')\n",
    "\n",
    "\n",
    "gen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1}\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aac6cd05-b62b-465a-8ecf-0678adc7297c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-08T07:23:29.111057Z",
     "iopub.status.busy": "2025-04-08T07:23:29.110756Z",
     "iopub.status.idle": "2025-04-08T07:23:29.116799Z",
     "shell.execute_reply": "2025-04-08T07:23:29.116325Z",
     "shell.execute_reply.started": "2025-04-08T07:23:29.111039Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你 想 去 蓟县 旅游 就 去 几次 ， 玩 了 七趟 你 想 。\n",
      "[['他 只要 想 去 蓟县 旅游 ， 就 会 去 玩 一趟 ， 前前后后 总共 去 了 七次 。']]\n",
      "------------------\n",
      "{'BLEU-1': 0.5926500877342693, 'BLEU-2': 0.5475035704705833, 'BLEU-3': 0.4625162625612449, 'BLEU-4': 0.3750708652558153, 'sacreBLEU': 0.2044036974896712, 'ROUGE-1': 0.6428571379591838, 'ROUGE-2': 0.19354838222684714, 'ROUGE-L': 0.5294117597750866, 'METEOR': 0.4427287581699346}\n"
     ]
    }
   ],
   "source": [
    "from eval import evaluate_all_metrics\n",
    "\n",
    "# 测试示例\n",
    "reference_text = [\"他只要想去蓟县旅游，就会去玩一趟，前前后后总共去了七次。\"]\n",
    "generated_text = \"你想去蓟县旅游就去几次，玩了七趟你想 。\"\n",
    "#reference_text = references[18]\n",
    "#generated_text = predictions[18]\n",
    "\n",
    "scores = evaluate_all_metrics(tokenizer, reference_text, generated_text, intensive=True) # 如果是中文请将intensive设置为True\n",
    "print(scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
